# -*- coding: utf-8 -*-
"""DataNeuron_Task

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YCbh1DeZclNHwu9AoriLQBWhLOr_anOr
"""

!pip install google-auth

import google.colab.auth

google.colab.auth.authenticate_user()

from google.colab import drive
drive.mount('/content/drive')

import numpy as np
import pandas as pd

import re
from tqdm import tqdm

import collections

from sklearn.cluster import KMeans

from nltk.stem import WordNetLemmatizer  # For Lemmetization of words
from nltk.corpus import stopwords  # Load list of stopwords
from nltk import word_tokenize # Convert paragraph in tokens

import pickle
import sys

from gensim.models import word2vec # For represent words in vectors
import gensim

file_path = '/content/drive/MyDrive/DataNeuron_Text_Similarity.csv'
text_data = pd.read_csv(file_path)
print("Shape of text_data : ", text_data.shape)
text_data.head(3)

text_data.isnull().sum() # Check if text data have any null values

# Preprocessing
def decontracted(phrase):
    # specific
    phrase = re.sub(r"won't", "will not", phrase)
    phrase = re.sub(r"can\'t", "can not", phrase)

    # general
    phrase = re.sub(r"n\'t", " not", phrase)
    phrase = re.sub(r"\'re", " are", phrase)
    phrase = re.sub(r"\'s", " is", phrase)
    phrase = re.sub(r"\'d", " would", phrase)
    phrase = re.sub(r"\'ll", " will", phrase)
    phrase = re.sub(r"\'t", " not", phrase)
    phrase = re.sub(r"\'ve", " have", phrase)
    phrase = re.sub(r"\'m", " am", phrase)
    return phrase

!pip install nltk

import nltk
nltk.download('stopwords')

# Combining all the above stundents

preprocessed_text1 = []

# tqdm is for printing the status bar

for sentance in tqdm(text_data['text1'].values):
    sent = decontracted(sentance)
    sent = sent.replace('\\r', ' ')
    sent = sent.replace('\\"', ' ')
    sent = sent.replace('\\n', ' ')
    sent = re.sub('[^A-Za-z0-9]+', ' ', sent)

    sent = ' '.join(e for e in sent.split() if e not in stopwords.words('english'))
    preprocessed_text1.append(sent.lower().strip())

# Merging preprocessed_text1 in text_data

text_data['text1'] = preprocessed_text1
text_data.head(3)

# Combining all the above stundents
from tqdm import tqdm
preprocessed_text2 = []

# tqdm is for printing the status bar
for sentance in tqdm(text_data['text2'].values):
    sent = decontracted(sentance)
    sent = sent.replace('\\r', ' ')
    sent = sent.replace('\\"', ' ')
    sent = sent.replace('\\n', ' ')
    sent = re.sub('[^A-Za-z0-9]+', ' ', sent)

    sent = ' '.join(e for e in sent.split() if e not in stopwords.words('english'))
    preprocessed_text2.append(sent.lower().strip())

# Merging preprocessed_text2 in text_data

text_data['text2'] = preprocessed_text2

text_data.head(3)

def word_tokenizer(text):
            #tokenizes and stems the text
            tokens = word_tokenize(text)
            lemmatizer = WordNetLemmatizer()
            tokens = [lemmatizer.lemmatize(t) for t in tokens]
            return tokens

import gensim

wordmodelfile='/content/drive/MyDrive/GoogleNews-vectors-negative300.bin.gz'
wordmodel= gensim.models.KeyedVectors.load_word2vec_format(wordmodelfile, binary=True)

!pip install nltk

import nltk
nltk.download('punkt')

from nltk.tokenize import word_tokenize

!pip install --upgrade gensim

from gensim.models import KeyedVectors

similarity = [] # List for store similarity score



for ind in text_data.index:

        s1 = text_data['text1'][ind]
        s2 = text_data['text2'][ind]

        if s1==s2:
                 similarity.append(0.0) # 0 means highly similar

        else:

            s1words = word_tokenize(s1)
            s2words = word_tokenize(s2)



            vocab = wordmodel.key_to_index #the vocabulary considered in the word embeddings

            if len(s1words and s2words)==0:
                    similarity.append(1.0)

            else:

                for word in s1words.copy(): #remove sentence words not found in the vocab
                    if (word not in vocab):


                            s1words.remove(word)


                for word in s2words.copy(): #idem

                    if (word not in vocab):

                            s2words.remove(word)


                similarity.append((1-wordmodel.n_similarity(s1words, s2words)))

import pandas as pd

# Get Unique_ID and similarity

final_score = pd.DataFrame({'Similarity_score':similarity})
final_score.head(3)

final_score.to_csv('final_score.csv',index=False)